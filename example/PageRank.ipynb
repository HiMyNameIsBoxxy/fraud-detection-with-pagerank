{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef68739c-49c5-4bdb-980b-ea6e041ade71",
   "metadata": {},
   "source": [
    "<h1>PageRank for Anomaly Detection</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9eb0a5e-19d3-4c2a-ac6d-0046d9753f70",
   "metadata": {},
   "source": [
    "The PageRank algorithm is a centrality measure that ranks the relative importance of nodes in a directed graph. It was invented by the founders of Google - Larry Page and Sergey Brin. The alogrithm was originally intended to rank webpages based on the simple idea that a page is important if many other important pages link to it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b67324d6-3a72-4810-91cc-51508cfb12c1",
   "metadata": {},
   "source": [
    "<h2>Concept</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5115fc2a-6803-4933-a86f-c421dc5119e0",
   "metadata": {},
   "source": [
    "Consider a person \"surfing\" random websites:\n",
    "1. The person starts on a random page.\n",
    "2. From the page they are currently on they can either:\n",
    "   * Click on a link to an external page, with probablity $\\alpha$ (usually 0.85)\n",
    "   * They can <i>teleport</i> (ie type in a url) to any random page, with probablity $1-\\alpha$\n",
    "3. After many hours of surfing, the probability of being on each page reaches a steady state (stabilizes). These steady state probabilities are the PageRank scores."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "464c5722-9f65-4d85-b5dd-ea89263316c5",
   "metadata": {},
   "source": [
    "<h2>Notation</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73b4a114-65af-4017-8fe1-d6a4ec4d293c",
   "metadata": {},
   "source": [
    "\n",
    "| Symbol | Meaning |\n",
    "|:--|:--|\n",
    "|  $N$  | Total number of nodes |\n",
    "|  $M$  | Column-stochastic adjacency matrix (each column sums to 1) |\n",
    "|  $M_{ij} = 1/k_j$  | Probability of moving from node $j$ to node $i$, where $k_j$ is the out-degree of node $j$ |\n",
    "|  $\\alpha \\in (0,1)$  | Damping factor (typically 0.85) |\n",
    "|  $\\mathbf{r}$  | PageRank vector (stationary distribution of node visit probabilities) |\n",
    "|  $\\mathbf{1}_N$  | Column vector of ones |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1824fff-7baf-42bc-af69-d5e6271c85c7",
   "metadata": {},
   "source": [
    "<h2>The PageRank Equation</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a49cca-9fd9-43a6-b9d3-55e9e429731d",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbf{r} \\;=\\; \\alpha\\, M\\, \\mathbf{r} \\;+\\; (1-\\alpha)\\,\\frac{1}{N}\\,\\mathbf{1}_N\n",
    "$$\n",
    "\n",
    "Equivalently, as a fixed point of the **Google matrix** $G$:\n",
    "$$\n",
    "\\mathbf{r} = G\\,\\mathbf{r}, \n",
    "\\qquad\n",
    "G \\;=\\; \\alpha\\,M \\;+\\; (1-\\alpha)\\,\\frac{1}{N}\\,E,\n",
    "$$\n",
    "where $E$ is the all-ones matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4e579dd-c0e0-4fbb-a108-920b9d393cd6",
   "metadata": {},
   "source": [
    "<h2>Iterative Computation</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb6c1d87-dd78-4227-8f12-b1d161178df0",
   "metadata": {},
   "source": [
    "Start from $\\,\\mathbf{r}^{(0)} = \\tfrac{1}{N}\\,\\boldsymbol{1}_N$ and iterate\n",
    "$$\n",
    "\\mathbf{r}^{(t+1)} \\;=\\; \\alpha\\, M\\, \\mathbf{r}^{(t)} \\;+\\; (1-\\alpha)\\,\\frac{1}{N}\\,\\boldsymbol{1}_N\n",
    "$$\n",
    "until convergence, e.g.\n",
    "$$\n",
    "\\left\\lVert \\mathbf{r}^{(t+1)} - \\mathbf{r}^{(t)} \\right\\rVert_1 \\;<\\; \\varepsilon .\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82550514-8374-468b-a908-e691fd38e1e1",
   "metadata": {},
   "source": [
    "<h2>Toy Example</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1416083b-cc04-4fce-a7ff-6b08e84620eb",
   "metadata": {},
   "source": [
    "Let’s take 3 pages: A, B, and C.\n",
    "* A links to B and C\n",
    "* B links to C\n",
    "* C links to A\n",
    "\n",
    "We can represent this as a graph:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c59c412d-c38b-49e4-ad52-5230f59d0225",
   "metadata": {},
   "source": [
    "$$A \\rightarrow B,C$$\n",
    "$$B \\rightarrow C$$\n",
    "$$C \\rightarrow A$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "418e040f-cd0d-4ef7-9cd9-74b8c7bddfdf",
   "metadata": {},
   "source": [
    "<h3>Step 1- Build the Transition Matrix</h3>\n",
    "<hr>\n",
    "Each column shows the probabilities from that node:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1611ac71-5e81-466b-91d3-d39a9bd63f20",
   "metadata": {},
   "source": [
    "| **To ↓ / From →** | **A** | **B** | **C** |\n",
    "|:------------------:|:----:|:----:|:----:|\n",
    "| **A** | 0 | 0 | 1 |\n",
    "| **B** | ½ | 0 | 0 |\n",
    "| **C** | ½ | 1 | 0 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f24e646c-fe46-4a89-9237-886f1f08e8d7",
   "metadata": {},
   "source": [
    "\n",
    "In matrix form:\n",
    "\n",
    "$$\n",
    "M =\n",
    "\\begin{bmatrix}\n",
    "0 & 0 & 1 \\\\\n",
    "\\frac{1}{2} & 0 & 0 \\\\\n",
    "\\frac{1}{2} & 1 & 0\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ced19295-440c-4ee1-8c22-fc325d33a61d",
   "metadata": {},
   "source": [
    "<h3>Step 2 - Iterative Update</h3>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f43e895e-8a5e-442b-8b17-77d1d3ebe320",
   "metadata": {},
   "source": [
    "We start with equal ranks for all nodes:\n",
    "\n",
    "$$\n",
    "\\mathbf{r}^{(0)} =\n",
    "\\begin{bmatrix}\n",
    "\\frac{1}{3} \\\\[4pt]\n",
    "\\frac{1}{3} \\\\[4pt]\n",
    "\\frac{1}{3}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "At each iteration, we update the rank vector using the PageRank equation:\n",
    "\n",
    "$$\n",
    "\\mathbf{r}^{(t+1)} = \\alpha\\, M\\, \\mathbf{r}^{(t)} + (1 - \\alpha)\\, \\frac{1}{N}\\, \\mathbb{1}_N\n",
    "$$\n",
    "\n",
    "where  \n",
    "\n",
    "- $\\alpha$ is the damping factor (typically $0.85$),  \n",
    "- $M$ is the column-stochastic transition matrix, and  \n",
    "- $\\mathbb{1}_N$ is the all-ones vector of length $N$.\n",
    "\n",
    "For this example with $N = 3$ and $\\alpha = 0.85$, the equation becomes:\n",
    "\n",
    "$$\n",
    "\\mathbf{r}^{(t+1)} = 0.85\\, M\\, \\mathbf{r}^{(t)} + 0.15\n",
    "\\begin{bmatrix}\n",
    "\\frac{1}{3} \\\\[4pt]\n",
    "\\frac{1}{3} \\\\[4pt]\n",
    "\\frac{1}{3}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "We repeat this process until the change between iterations is small:\n",
    "\n",
    "$$\n",
    "\\left\\lVert \\mathbf{r}^{(t+1)} - \\mathbf{r}^{(t)} \\right\\rVert_1 < \\varepsilon\n",
    "$$\n",
    "\n",
    "At convergence, $\\mathbf{r}^{(t)}$ gives the **steady-state PageRank scores** for all nodes.\n",
    "\n",
    "After several iterations, the ranks converge approximately to:\n",
    "\n",
    "$$\n",
    "\\mathbf{r}^{(T)} =\n",
    "\\begin{bmatrix}\n",
    "0.387 \\\\[4pt]\n",
    "0.214 \\\\[4pt]\n",
    "0.399\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "This tells us:\n",
    "* C is the most important\n",
    "* A is the 2nd most important\n",
    "* B is the least important"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2fe2c0e-2703-4494-bc89-3e8b034f1234",
   "metadata": {},
   "source": [
    "<h2>Implementing PageRank</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "881343b2-2638-49af-a07e-e78aed86a4d1",
   "metadata": {},
   "source": [
    "Here's a simple implementation that only uses Numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfc8ecba-e70a-4d21-be12-d996c6d9e292",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "\n",
    "def pagerank_simple(graph, damping_factor=0.85, max_iterations=100, tol=1e-6):    \n",
    "    nodes = list(graph.keys())\n",
    "    n = len(nodes)\n",
    "    ranks = {node: 1/n for node in nodes} # initial rank for each node\n",
    "    prob = np.array([1/n for node in nodes])\n",
    "    new_ranks = ranks.copy()\n",
    "    \n",
    "    # Create a mapping from node to index for easier computation\n",
    "    node_to_index = {node: i for i, node in enumerate(nodes)}\n",
    "    index_to_node = {i: node for node, i in node_to_index.items()}\n",
    "    \n",
    "    # Build the adjacency matrix\n",
    "    adjacency_matrix = np.zeros((n, n))\n",
    "    for node, neighbors in graph.items():\n",
    "        if neighbors: # avoid division by zero\n",
    "            for neighbor in neighbors:\n",
    "                adjacency_matrix[node_to_index[neighbor], node_to_index[node]] = 1 / len(neighbors)\n",
    "    \n",
    "    # Iterative computation\n",
    "    k = ((1 - damping_factor) / n)*np.ones(n)\n",
    "    Mprime = damping_factor * adjacency_matrix\n",
    "    prob = np.linalg.inv(Mprime-np.eye(n))@-k.T\n",
    "    ranks = dict(zip(ranks.keys(), prob))\n",
    "\n",
    "    return ranks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a9a47b-68f4-4db2-96df-57a49d271519",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = {\n",
    "        'A': ['B', 'C'],\n",
    "        'B': ['C'],\n",
    "        'C': ['A']        \n",
    "    }\n",
    "ranks = pagerank_simple(graph)\n",
    "print(\"PageRank scores from Simple Implementation:\")\n",
    "for node, score in sorted(ranks.items()):\n",
    "    print(f\"  {node}: {score:.6f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3559583-6434-49fe-9dd3-4ddfcf04d68c",
   "metadata": {},
   "source": [
    "We can also use the NetworkX PageRank implementation to compute the ranks for us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b08d307-ae3d-4ccc-8b6d-bad716560bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install networkx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dbdab71-5519-4f32-afdf-bcdc03ab06fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "\n",
    "# Create directed graph\n",
    "G = nx.DiGraph(graph)\n",
    "\n",
    "# Compute PageRank\n",
    "ranks = nx.pagerank(G, alpha=0.85)\n",
    "\n",
    "print(\"PageRank scores from NetworkX:\")\n",
    "for node, score in sorted(ranks.items()):\n",
    "    print(f\"  {node}: {score:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b07f71f-c88e-4882-b01e-75a787990622",
   "metadata": {},
   "source": [
    "<h2>Example with Amazon e-commerce data</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab966b3-4511-47d0-9be2-65146ce94ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load directed graph\n",
    "path = \"amazon0302.txt\"\n",
    "G = nx.read_edgelist(path, comments=\"#\", create_using=nx.DiGraph(), nodetype=int)\n",
    "\n",
    "print(f\"Nodes: {G.number_of_nodes():,}\")\n",
    "print(f\"Edges: {G.number_of_edges():,}\")\n",
    "\n",
    "# Compute PageRank\n",
    "pr = nx.pagerank(G, alpha=0.85)\n",
    "\n",
    "# Show top-10 nodes\n",
    "top10 = sorted(pr.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "for node, score in top10:\n",
    "    print(f\"Product {node:>6} → PageRank={score:.6f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d9a273-d5bc-4d44-92c3-37fa7860f48c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pyvis\n",
    "#!pip install jinja2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "066e74a2-d39b-45ef-a638-7bea15a196fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, webbrowser\n",
    "import numpy as np\n",
    "from pyvis.network import Network\n",
    "\n",
    "N = 400  # keep this reasonable for the browser\n",
    "top_nodes = sorted(pr, key=pr.get, reverse=True)[:N]\n",
    "subG = G.subgraph(top_nodes).copy()\n",
    "\n",
    "# Setup\n",
    "net = Network(height=\"800px\", width=\"100%\", directed=True,\n",
    "              bgcolor=\"#1e1e1e\", font_color=\"white\", notebook=False)\n",
    "\n",
    "# add nodes\n",
    "vals = np.array([pr[n] for n in subG.nodes()])\n",
    "vmin, vmax = vals.min(), vals.max()\n",
    "def size_for(v):\n",
    "    if vmax == vmin: return 10.0\n",
    "    z = (v - vmin) / (vmax - vmin)\n",
    "    return 6.0 + 34.0 * z\n",
    "\n",
    "# simple buckets/colors\n",
    "q99, q95, q80 = np.quantile(vals, [0.99, 0.95, 0.80])\n",
    "def color_for(v):\n",
    "    if v >= q99: return \"#e74c3c\"\n",
    "    if v >= q95: return \"#e67e22\"\n",
    "    if v >= q80: return \"#f1c40f\"\n",
    "    return \"#4aa3f0\"\n",
    "\n",
    "for n in subG.nodes():\n",
    "    v = pr[n]\n",
    "    net.add_node(n,\n",
    "                 label=str(n),\n",
    "                 title=f\"Node {n}<br>PageRank={v:.6g}\",\n",
    "                 size=size_for(v),\n",
    "                 color=color_for(v))\n",
    "\n",
    "# add edges\n",
    "for u, v in subG.edges():\n",
    "    net.add_edge(u, v)\n",
    "\n",
    "# Write HTML and open \n",
    "outfile = \"amazon_network.html\"\n",
    "net.write_html(outfile)  \n",
    "webbrowser.open(\"file://\" + os.path.abspath(outfile))\n",
    "print(f\"Wrote {outfile} and opened in your browser.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82cab702-e10c-436e-8c23-b93f4c0a87e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to arrays\n",
    "nodes = np.fromiter(pr.keys(), dtype=int, count=len(pr))\n",
    "scores = np.fromiter(pr.values(), dtype=float, count=len(pr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaeefe1c-6f2f-49e0-b3d9-0e3c8bc0456e",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 20\n",
    "alpha=0.85\n",
    "order = np.argsort(-scores)[:k]  # top-k by PR\n",
    "print(\"\\n=== Top nodes by PageRank ===\")\n",
    "print(f\"(alpha={alpha})\\n\")\n",
    "print(f\"{'Rank':>4}  {'Node':>10}  {'PR':>12}  {'in_deg':>8}  {'out_deg':>8}  {'total_deg':>9}\")\n",
    "for r, idx in enumerate(order, start=1):\n",
    "    n = nodes[idx]\n",
    "    score = scores[idx]\n",
    "    indeg = G.in_degree(n)\n",
    "    outdeg = G.out_degree(n)\n",
    "    print(f\"{r:>4}  {n:>10}  {score:>12.6g}  {indeg:>8}  {outdeg:>8}  {indeg+outdeg:>9}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3920ddab-7767-46df-8b8c-b67029ef213e",
   "metadata": {},
   "source": [
    "From the table above, it appears there is a correlation with the number of in-degree edges and the final pagerank score."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce653be6-ea41-4ec5-8ef7-b7b86c1b2177",
   "metadata": {},
   "source": [
    "<h2>Repurposing PageRank to Flag Anomalies</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38bf797b-b3fc-4fc7-a3e6-7804c411f73d",
   "metadata": {},
   "source": [
    "The PageRank algorithm has been repurposed quite frequently to detect anomalies. One of the readings in this week's homework assignment involves applying PageRank to nursing home residents movements to detect movements that may be a cause for concern. Here are a few common ways you can apply PageRank to find anomalies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7a1bd7b-5d39-4b98-9995-a699570442d7",
   "metadata": {},
   "source": [
    "<h3>Outliers in the Ranks</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54e84c8a-1057-45f9-9293-6706a8129d85",
   "metadata": {},
   "source": [
    "In our dataset we have $N=262,111$ nodes. If each node were equally important, this implies each node's expected PageRank would be $1/N$ or $1/262,111 \\approx 3.8\\times10^{-6}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0c78808-980e-4b52-b5cb-e8a360070ce3",
   "metadata": {},
   "source": [
    "Nodes whose PageRank score is extremely high/low relative to the global distribution are flagged. Since our PageRank results are highly skewed, we opt to flag the top and bottom 0.5% of scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "204556d1-ab8e-4600-9243-9f2728df92de",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = np.fromiter(pr.values(), float)\n",
    "\n",
    "# get absolute cutoff thresholds\n",
    "top_cutoff    = np.percentile(scores, 99.5)   # top 0.5%\n",
    "bottom_cutoff = np.percentile(scores, 0.5)    # bottom 0.5%\n",
    "\n",
    "top_nodes = [n for n,s in pr.items() if s >= top_cutoff]\n",
    "bottom_nodes = [n for n,s in pr.items() if s <= bottom_cutoff]\n",
    "\n",
    "print(\"Expected avg PR:\", 1 / G.number_of_nodes())\n",
    "print(\"Top 0.5% cutoff:\", top_cutoff)\n",
    "print(\"Bottom 0.5% cutoff:\", bottom_cutoff)\n",
    "print(\"Num top:\", len(top_nodes))\n",
    "print(\"Num bottom:\", len(bottom_nodes))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2431fbc6-70ae-4c01-9f82-dbae95fd32a8",
   "metadata": {},
   "source": [
    "<h3>PageRank vs. Degree \"mismatch\" (residual anomalies)</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c399c2b-85bf-445e-8c4a-6993dcf61cf4",
   "metadata": {},
   "source": [
    "If we plot the PageRank scores with In-Degree, we usually observe a significant positive correlation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b2810a-de1a-4d9a-84c3-c1b2eeeffe4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "nodes  = list(pr.keys())\n",
    "pr_vals = np.array([pr[n] for n in nodes])\n",
    "in_deg  = np.array([G.in_degree(n) for n in nodes], dtype=float)\n",
    "\n",
    "# ---- (1) Raw scatter ----\n",
    "plt.figure(figsize=(7,5))\n",
    "plt.scatter(in_deg, pr_vals, s=8, alpha=0.5)\n",
    "plt.xlabel(\"In-degree\")\n",
    "plt.ylabel(\"PageRank score\")\n",
    "plt.title(\"PageRank vs In-degree\")\n",
    "plt.grid(True, ls=\"--\", alpha=0.3)\n",
    "plt.tight_layout()\n",
    "#plt.show()\n",
    "\n",
    "a, b = np.polyfit(in_deg, pr_vals, 1)\n",
    "x_line = np.linspace(in_deg.min(), in_deg.max(), 200)\n",
    "plt.plot(x_line, a*x_line + b, color=\"red\", lw=2)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfc26641-3246-420e-9efe-68d727fa3cf2",
   "metadata": {},
   "source": [
    "The data is \"fanned-out\" at higher degrees and it justifies taking logarithms of both variables. However we will ignore this for now to illustrate the methodology."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f864c7a-2f0f-4429-9c47-8cca27ddf608",
   "metadata": {},
   "outputs": [],
   "source": [
    "in_degs = np.array([G.in_degree(n) for n in nodes], dtype=float)\n",
    "corr = np.corrcoef(in_degs, scores)[0,1]\n",
    "print(f\"\\nCorrelation(PR, in-degree) ≈ {corr:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f66ac638-6e7f-459b-baca-a5e9ec652d6f",
   "metadata": {},
   "source": [
    "Nodes with a much higher PageRank than their in-degree predicts (or vice-versa) can be suspicious (e.g., boosted by a few very influential neighbors). To do this we regress PageRank on in_degree and inspect residuals. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "875581d5-0967-4992-9e15-0436b45598f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code will take 5-10 minutes to run\n",
    "import numpy as np\n",
    "\n",
    "nodes = list(G.nodes())\n",
    "indeg = np.array([G.in_degree(n) for n in nodes], float)\n",
    "X = indeg\n",
    "y = np.array([pr[n] for n in nodes])\n",
    "\n",
    "# simple linear fit\n",
    "a, b = np.polyfit(X, y, 1)\n",
    "resid = y - (a*X + b)\n",
    "\n",
    "# anomalies: PR too high/low vs. degree\n",
    "hi_resid = [n for n,r in zip(nodes,resid) if r > 4*resid.std()]  # fewer than expected in-degrees but neighbors are very influential\n",
    "lo_resid = [n for n,r in zip(nodes,resid) if r < -4*resid.std()] # several in-degrees but very few are important\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9534dbe1-6585-4633-8a15-af6531031a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Nodes with the highest residuals:{hi_resid[:10]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "545a68d4-7c7b-4047-a96d-d3ba82882283",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c85766-1503-487c-991a-6f5c91be31e2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
